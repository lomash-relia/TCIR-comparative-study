{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e540a9bd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T21:56:31.055533Z",
     "iopub.status.busy": "2024-11-22T21:56:31.055239Z",
     "iopub.status.idle": "2024-11-23T08:20:18.798518Z",
     "shell.execute_reply": "2024-11-23T08:20:18.797440Z"
    },
    "papermill": {
     "duration": 37427.749399,
     "end_time": "2024-11-23T08:20:18.800685",
     "exception": false,
     "start_time": "2024-11-22T21:56:31.051286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total Samples - Train: 9532, Validation: 2042, Test: 2044\n",
      "\n",
      "Training CONVNEXT_TINY...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
      "100%|██████████| 109M/109M [00:00<00:00, 208MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 33.784, Val Loss: 24.944\n",
      "Epoch [2/100], Train Loss: 22.174, Val Loss: 21.360\n",
      "Epoch [3/100], Train Loss: 21.087, Val Loss: 21.221\n",
      "Epoch [4/100], Train Loss: 19.827, Val Loss: 18.778\n",
      "Epoch [5/100], Train Loss: 17.551, Val Loss: 16.348\n",
      "Epoch [6/100], Train Loss: 15.404, Val Loss: 14.616\n",
      "Epoch [7/100], Train Loss: 13.603, Val Loss: 12.431\n",
      "Epoch [8/100], Train Loss: 11.908, Val Loss: 11.041\n",
      "Epoch [9/100], Train Loss: 10.950, Val Loss: 9.892\n",
      "Epoch [10/100], Train Loss: 10.219, Val Loss: 9.637\n",
      "Epoch [11/100], Train Loss: 9.698, Val Loss: 9.561\n",
      "Epoch [12/100], Train Loss: 9.381, Val Loss: 9.118\n",
      "Epoch [13/100], Train Loss: 9.061, Val Loss: 9.813\n",
      "Epoch [14/100], Train Loss: 8.830, Val Loss: 10.096\n",
      "Epoch [15/100], Train Loss: 8.733, Val Loss: 9.132\n",
      "Epoch [16/100], Train Loss: 8.528, Val Loss: 9.157\n",
      "Epoch [17/100], Train Loss: 8.406, Val Loss: 8.789\n",
      "Epoch [18/100], Train Loss: 8.240, Val Loss: 8.342\n",
      "Epoch [19/100], Train Loss: 8.135, Val Loss: 8.617\n",
      "Epoch [20/100], Train Loss: 8.031, Val Loss: 8.322\n",
      "Epoch [21/100], Train Loss: 7.882, Val Loss: 8.332\n",
      "Epoch [22/100], Train Loss: 7.716, Val Loss: 8.872\n",
      "Epoch [23/100], Train Loss: 7.625, Val Loss: 8.392\n",
      "Epoch [24/100], Train Loss: 7.759, Val Loss: 8.049\n",
      "Epoch [25/100], Train Loss: 7.484, Val Loss: 8.296\n",
      "Epoch [26/100], Train Loss: 7.334, Val Loss: 8.107\n",
      "Epoch [27/100], Train Loss: 7.190, Val Loss: 7.969\n",
      "Epoch [28/100], Train Loss: 7.199, Val Loss: 8.681\n",
      "Epoch [29/100], Train Loss: 6.956, Val Loss: 8.112\n",
      "Epoch [30/100], Train Loss: 6.955, Val Loss: 7.927\n",
      "Epoch [31/100], Train Loss: 6.875, Val Loss: 8.075\n",
      "Epoch [32/100], Train Loss: 6.775, Val Loss: 8.145\n",
      "Epoch [33/100], Train Loss: 6.669, Val Loss: 7.698\n",
      "Epoch [34/100], Train Loss: 6.551, Val Loss: 7.909\n",
      "Epoch [35/100], Train Loss: 6.404, Val Loss: 7.907\n",
      "Epoch [36/100], Train Loss: 6.314, Val Loss: 7.877\n",
      "Epoch [37/100], Train Loss: 6.316, Val Loss: 8.145\n",
      "Epoch [38/100], Train Loss: 6.210, Val Loss: 7.452\n",
      "Epoch [39/100], Train Loss: 6.031, Val Loss: 7.635\n",
      "Epoch [40/100], Train Loss: 6.057, Val Loss: 7.666\n",
      "Epoch [41/100], Train Loss: 5.945, Val Loss: 7.984\n",
      "Epoch [42/100], Train Loss: 5.702, Val Loss: 7.608\n",
      "Epoch [43/100], Train Loss: 5.720, Val Loss: 7.609\n",
      "Epoch [44/100], Train Loss: 5.680, Val Loss: 7.543\n",
      "Epoch [45/100], Train Loss: 5.141, Val Loss: 7.439\n",
      "Epoch [46/100], Train Loss: 4.882, Val Loss: 7.311\n",
      "Epoch [47/100], Train Loss: 4.812, Val Loss: 7.222\n",
      "Epoch [48/100], Train Loss: 4.727, Val Loss: 7.300\n",
      "Epoch [49/100], Train Loss: 4.616, Val Loss: 7.305\n",
      "Epoch [50/100], Train Loss: 4.561, Val Loss: 7.296\n",
      "Epoch [51/100], Train Loss: 4.545, Val Loss: 7.316\n",
      "Epoch [52/100], Train Loss: 4.502, Val Loss: 7.233\n",
      "Epoch [53/100], Train Loss: 4.463, Val Loss: 7.249\n",
      "Epoch [54/100], Train Loss: 4.469, Val Loss: 7.238\n",
      "Epoch [55/100], Train Loss: 4.361, Val Loss: 7.243\n",
      "Epoch [56/100], Train Loss: 4.413, Val Loss: 7.238\n",
      "Epoch [57/100], Train Loss: 4.350, Val Loss: 7.251\n",
      "Epoch [58/100], Train Loss: 4.366, Val Loss: 7.261\n",
      "Epoch [59/100], Train Loss: 4.330, Val Loss: 7.246\n",
      "Epoch [60/100], Train Loss: 4.360, Val Loss: 7.248\n",
      "Epoch [61/100], Train Loss: 4.337, Val Loss: 7.251\n",
      "Epoch [62/100], Train Loss: 4.342, Val Loss: 7.253\n",
      "Early stopping triggered\n",
      "\n",
      "Evaluating CONVNEXT_TINY on test set...\n",
      "\n",
      "CONVNEXT_TINY - Test MAE: 7.491, RMSE: 10.185, R2 Score: 0.873\n",
      "\n",
      "Calculating FLOPs and Parameters for CONVNEXT_TINY...\n",
      "\n",
      "CONVNEXT_TINY - Total Params: 27,820,897, Total FLOPs: 4,469,670,144\n",
      "\n",
      "Training history saved to 'training_history_convnext_tiny.csv'\n",
      "\n",
      "Training CONVNEXT_SMALL...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_small-0c510722.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small-0c510722.pth\n",
      "100%|██████████| 192M/192M [00:03<00:00, 56.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 33.576, Val Loss: 25.080\n",
      "Epoch [2/100], Train Loss: 22.279, Val Loss: 21.437\n",
      "Epoch [3/100], Train Loss: 20.956, Val Loss: 20.739\n",
      "Epoch [4/100], Train Loss: 18.751, Val Loss: 17.209\n",
      "Epoch [5/100], Train Loss: 15.738, Val Loss: 14.323\n",
      "Epoch [6/100], Train Loss: 13.822, Val Loss: 12.698\n",
      "Epoch [7/100], Train Loss: 12.301, Val Loss: 12.553\n",
      "Epoch [8/100], Train Loss: 11.036, Val Loss: 10.230\n",
      "Epoch [9/100], Train Loss: 10.264, Val Loss: 9.960\n",
      "Epoch [10/100], Train Loss: 9.725, Val Loss: 9.508\n",
      "Epoch [11/100], Train Loss: 9.485, Val Loss: 9.290\n",
      "Epoch [12/100], Train Loss: 9.164, Val Loss: 9.109\n",
      "Epoch [13/100], Train Loss: 8.999, Val Loss: 9.282\n",
      "Epoch [14/100], Train Loss: 8.620, Val Loss: 8.969\n",
      "Epoch [15/100], Train Loss: 8.602, Val Loss: 8.899\n",
      "Epoch [16/100], Train Loss: 8.444, Val Loss: 8.429\n",
      "Epoch [17/100], Train Loss: 8.412, Val Loss: 8.145\n",
      "Epoch [18/100], Train Loss: 8.098, Val Loss: 9.168\n",
      "Epoch [19/100], Train Loss: 8.128, Val Loss: 8.180\n",
      "Epoch [20/100], Train Loss: 7.974, Val Loss: 8.293\n",
      "Epoch [21/100], Train Loss: 7.863, Val Loss: 8.178\n",
      "Epoch [22/100], Train Loss: 7.653, Val Loss: 8.598\n",
      "Epoch [23/100], Train Loss: 7.648, Val Loss: 7.848\n",
      "Epoch [24/100], Train Loss: 7.432, Val Loss: 8.071\n",
      "Epoch [25/100], Train Loss: 7.408, Val Loss: 8.086\n",
      "Epoch [26/100], Train Loss: 7.290, Val Loss: 7.951\n",
      "Epoch [27/100], Train Loss: 7.180, Val Loss: 7.928\n",
      "Epoch [28/100], Train Loss: 7.175, Val Loss: 7.854\n",
      "Epoch [29/100], Train Loss: 6.959, Val Loss: 7.820\n",
      "Epoch [30/100], Train Loss: 6.903, Val Loss: 7.955\n",
      "Epoch [31/100], Train Loss: 6.779, Val Loss: 7.767\n",
      "Epoch [32/100], Train Loss: 6.756, Val Loss: 7.981\n",
      "Epoch [33/100], Train Loss: 6.649, Val Loss: 7.828\n",
      "Epoch [34/100], Train Loss: 6.476, Val Loss: 7.725\n",
      "Epoch [35/100], Train Loss: 6.506, Val Loss: 7.698\n",
      "Epoch [36/100], Train Loss: 6.402, Val Loss: 8.007\n",
      "Epoch [37/100], Train Loss: 6.158, Val Loss: 7.880\n",
      "Epoch [38/100], Train Loss: 6.193, Val Loss: 7.713\n",
      "Epoch [39/100], Train Loss: 6.072, Val Loss: 8.029\n",
      "Epoch [40/100], Train Loss: 5.923, Val Loss: 8.282\n",
      "Epoch [41/100], Train Loss: 5.881, Val Loss: 7.809\n",
      "Epoch [42/100], Train Loss: 5.328, Val Loss: 7.439\n",
      "Epoch [43/100], Train Loss: 5.138, Val Loss: 7.403\n",
      "Epoch [44/100], Train Loss: 5.020, Val Loss: 7.425\n",
      "Epoch [45/100], Train Loss: 4.891, Val Loss: 7.393\n",
      "Epoch [46/100], Train Loss: 4.844, Val Loss: 7.355\n",
      "Epoch [47/100], Train Loss: 4.778, Val Loss: 7.389\n",
      "Epoch [48/100], Train Loss: 4.757, Val Loss: 7.322\n",
      "Epoch [49/100], Train Loss: 4.690, Val Loss: 7.354\n",
      "Epoch [50/100], Train Loss: 4.680, Val Loss: 7.370\n",
      "Epoch [51/100], Train Loss: 4.674, Val Loss: 7.353\n",
      "Epoch [52/100], Train Loss: 4.622, Val Loss: 7.280\n",
      "Epoch [53/100], Train Loss: 4.588, Val Loss: 7.334\n",
      "Epoch [54/100], Train Loss: 4.566, Val Loss: 7.339\n",
      "Epoch [55/100], Train Loss: 4.500, Val Loss: 7.335\n",
      "Epoch [56/100], Train Loss: 4.503, Val Loss: 7.425\n",
      "Epoch [57/100], Train Loss: 4.454, Val Loss: 7.321\n",
      "Epoch [58/100], Train Loss: 4.377, Val Loss: 7.239\n",
      "Epoch [59/100], Train Loss: 4.384, Val Loss: 7.314\n",
      "Epoch [60/100], Train Loss: 4.287, Val Loss: 7.324\n",
      "Epoch [61/100], Train Loss: 4.329, Val Loss: 7.286\n",
      "Epoch [62/100], Train Loss: 4.315, Val Loss: 7.288\n",
      "Epoch [63/100], Train Loss: 4.193, Val Loss: 7.327\n",
      "Epoch [64/100], Train Loss: 4.239, Val Loss: 7.328\n",
      "Epoch [65/100], Train Loss: 4.171, Val Loss: 7.270\n",
      "Epoch [66/100], Train Loss: 4.177, Val Loss: 7.284\n",
      "Epoch [67/100], Train Loss: 4.164, Val Loss: 7.282\n",
      "Epoch [68/100], Train Loss: 4.211, Val Loss: 7.273\n",
      "Epoch [69/100], Train Loss: 4.170, Val Loss: 7.287\n",
      "Epoch [70/100], Train Loss: 4.151, Val Loss: 7.269\n",
      "Epoch [71/100], Train Loss: 4.106, Val Loss: 7.270\n",
      "Epoch [72/100], Train Loss: 4.092, Val Loss: 7.270\n",
      "Epoch [73/100], Train Loss: 4.096, Val Loss: 7.273\n",
      "Early stopping triggered\n",
      "\n",
      "Evaluating CONVNEXT_SMALL on test set...\n",
      "\n",
      "CONVNEXT_SMALL - Test MAE: 7.372, RMSE: 10.083, R2 Score: 0.875\n",
      "\n",
      "Calculating FLOPs and Parameters for CONVNEXT_SMALL...\n",
      "\n",
      "CONVNEXT_SMALL - Total Params: 49,455,457, Total FLOPs: 8,704,624,896\n",
      "\n",
      "Training history saved to 'training_history_convnext_small.csv'\n",
      "\n",
      "Final Results:\n",
      "\n",
      "            Model       MAE       RMSE  R2 Score  Total Params  Total FLOPs\n",
      "0   CONVNEXT_TINY  7.490792  10.185383  0.872521      27820897   4469670144\n",
      "1  CONVNEXT_SMALL  7.372137  10.082619  0.875081      49455457   8704624896\n",
      "\n",
      "Model evaluation results saved to 'model_evaluation_results_experiment2_convnext.csv'\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Part 1: Data Preparation and Dataset Class\n",
    "# ======================================\n",
    "\n",
    "# ======================================\n",
    "# Import Necessary Libraries\n",
    "# ======================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from torchvision import models\n",
    "\n",
    "# Install necessary packages if not already installed\n",
    "try:\n",
    "    from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "except ImportError:\n",
    "    !pip install fvcore -q\n",
    "    from fvcore.nn import FlopCountAnalysis, parameter_count\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ======================================\n",
    "# Reproducibility\n",
    "# ======================================\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(42)\n",
    "\n",
    "# ======================================\n",
    "# Device Configuration\n",
    "# ======================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# ======================================\n",
    "# Data Preparation\n",
    "# ======================================\n",
    "# File path to the dataset\n",
    "data_path = '/kaggle/input/tcir-cpac-io-sh-h5-file/TCIR-CPAC_IO_SH.h5'\n",
    "\n",
    "# Load Dataset Information and Filter\n",
    "try:\n",
    "    data_info = pd.read_hdf(data_path, key=\"info\", mode='r')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading HDF5 file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Filter for the 'SH' dataset\n",
    "data_info_filtered = data_info[data_info['data_set'].isin(['SH'])]\n",
    "\n",
    "# Undersampling for Balanced Dataset\n",
    "low_vmax_threshold = np.percentile(data_info_filtered['Vmax'].values, 35)\n",
    "low_vmax_indices = data_info_filtered[\n",
    "    data_info_filtered['Vmax'] <= low_vmax_threshold].index\n",
    "remaining_indices = data_info_filtered[\n",
    "    data_info_filtered['Vmax'] > low_vmax_threshold].index\n",
    "undersample_ratio = 0.3\n",
    "undersample_size = int(len(low_vmax_indices) * undersample_ratio) \\\n",
    "    if len(low_vmax_indices) > 0 else 0\n",
    "\n",
    "if undersample_size > 0:\n",
    "    undersample_indices = np.random.choice(\n",
    "        low_vmax_indices, undersample_size, replace=False)\n",
    "    balanced_indices = np.concatenate(\n",
    "        (undersample_indices, remaining_indices))\n",
    "else:\n",
    "    balanced_indices = remaining_indices\n",
    "\n",
    "data_info_balanced = data_info_filtered.loc[\n",
    "    balanced_indices].reset_index()\n",
    "\n",
    "# Define Transformations\n",
    "# Separate transforms for InceptionV3 (input size 299x299)\n",
    "common_train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(degrees=(0, 360), fill=0),\n",
    "    transforms.CenterCrop(size=(152, 152)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "common_val_test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(size=(152, 152)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "inception_train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(degrees=(0, 360), fill=0),\n",
    "    transforms.CenterCrop(size=(152, 152)),\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "inception_val_test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(size=(152, 152)),\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset Class\n",
    "class TCIRLazyDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, data_info, channels=[0, 1, 3],\n",
    "                 transform=None):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.data_info = data_info\n",
    "        self.channels = channels\n",
    "        self.transform = transform\n",
    "        self.channel_norm_values = {0: 350, 1: 275, 3: 4.35}\n",
    "        try:\n",
    "            self.hf = h5py.File(self.hdf5_file, 'r')\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening HDF5 file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            hdf5_index = self.data_info.at[idx, 'index']\n",
    "            data_matrix = self.hf['matrix'][hdf5_index, :, :, self.channels]\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing data at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Normalize image data\n",
    "        image = np.empty_like(data_matrix, dtype=np.float32)\n",
    "        for i, ch in enumerate(self.channels):\n",
    "            norm_value = self.channel_norm_values.get(ch, 1.0)\n",
    "            channel_data = data_matrix[:, :, i]\n",
    "            channel_data = np.clip(channel_data, None, norm_value)\n",
    "            image[:, :, i] = np.nan_to_num(channel_data / norm_value)\n",
    "\n",
    "        # Convert to tensor and apply transformations\n",
    "        image = torch.tensor(image).permute(2, 0, 1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Retrieve the label (Vmax)\n",
    "        label = torch.tensor(\n",
    "            self.data_info.at[idx, 'Vmax'], dtype=torch.float32)\n",
    "        return image, label\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'hf') and self.hf:\n",
    "            self.hf.close()\n",
    "\n",
    "# Split Dataset\n",
    "full_dataset_size = len(data_info_balanced)\n",
    "indices = list(range(full_dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.7 * full_dataset_size)\n",
    "val_size = int(0.15 * full_dataset_size)\n",
    "test_size = full_dataset_size - train_size - val_size\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_data_info = data_info_balanced.iloc[\n",
    "    train_indices].reset_index(drop=True)\n",
    "val_data_info = data_info_balanced.iloc[\n",
    "    val_indices].reset_index(drop=True)\n",
    "test_data_info = data_info_balanced.iloc[\n",
    "    test_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Total Samples - Train: {len(train_data_info)}, '\n",
    "      f'Validation: {len(val_data_info)}, Test: {len(test_data_info)}')\n",
    "\n",
    "# ======================================\n",
    "# Part 2: Model Definitions\n",
    "# ======================================\n",
    "\n",
    "# ======================================\n",
    "# Function to Create Models Dynamically\n",
    "# ======================================\n",
    "def create_model(architecture, num_channels=3):\n",
    "    \"\"\"\n",
    "    Creates a model based on the specified architecture.\n",
    "\n",
    "    Args:\n",
    "        architecture (str): Name of the architecture.\n",
    "        num_channels (int): Number of input channels.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The constructed model.\n",
    "        train_transform (Compose): Transformation for training data.\n",
    "        val_test_transform (Compose): Transformation for validation/test data.\n",
    "        input_size (tuple): Input size expected by the model.\n",
    "    \"\"\"\n",
    "    # Define a mapping from architecture to weights enum\n",
    "    weights_dict = {\n",
    "        # 'efficientnet_v2_s': models.EfficientNet_V2_S_Weights.DEFAULT,\n",
    "        # 'efficientnet_v2_m': models.EfficientNet_V2_M_Weights.DEFAULT,\n",
    "        # 'inception_v3': models.Inception_V3_Weights.DEFAULT,\n",
    "        'convnext_tiny': models.ConvNeXt_Tiny_Weights.DEFAULT,\n",
    "        'convnext_small': models.ConvNeXt_Small_Weights.DEFAULT\n",
    "    }\n",
    "    \n",
    "    if architecture not in weights_dict:\n",
    "        raise ValueError(f\"Architecture {architecture} not supported.\")\n",
    "    \n",
    "    weights = weights_dict[architecture]\n",
    "    \n",
    "    if architecture in ['efficientnet_v2_s', 'efficientnet_v2_m']:\n",
    "        # Load the base model with pretrained weights\n",
    "        base_model = getattr(models, architecture)(weights=weights)\n",
    "        # Modify the first conv layer if num_channels != 3\n",
    "        if num_channels != 3:\n",
    "            base_model.features[0][0] = nn.Conv2d(\n",
    "                num_channels,\n",
    "                base_model.features[0][0].out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        # Modify the classifier\n",
    "        in_features = base_model.classifier[1].in_features\n",
    "        base_model.classifier[1] = nn.Linear(in_features, 1)\n",
    "        model = base_model\n",
    "        # Use common transforms\n",
    "        train_transform = common_train_transform\n",
    "        val_test_transform = common_val_test_transform\n",
    "        input_size = (num_channels, 224, 224)\n",
    "\n",
    "    elif architecture == 'inception_v3':\n",
    "        # Load the base model with pretrained weights\n",
    "        base_model = models.inception_v3(weights=weights)\n",
    "        # Modify the first conv layer if num_channels != 3\n",
    "        if num_channels != 3:\n",
    "            base_model.Conv2d_1a_3x3.conv = nn.Conv2d(\n",
    "                num_channels,\n",
    "                base_model.Conv2d_1a_3x3.conv.out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                bias=False)\n",
    "        # Modify the classifier\n",
    "        in_features = base_model.fc.in_features\n",
    "        base_model.fc = nn.Linear(in_features, 1)\n",
    "        # Set aux_logits to False if not needed\n",
    "        base_model.aux_logits = False\n",
    "        model = base_model\n",
    "        # Use Inception-specific transforms\n",
    "        train_transform = inception_train_transform\n",
    "        val_test_transform = inception_val_test_transform\n",
    "        input_size = (num_channels, 299, 299)\n",
    "\n",
    "    elif architecture in ['convnext_tiny', 'convnext_small']:\n",
    "        # Load the base model with pretrained weights\n",
    "        base_model = getattr(models, architecture)(weights=weights)\n",
    "        # Modify the first conv layer if num_channels != 3\n",
    "        if num_channels != 3:\n",
    "            base_model.features[0][0] = nn.Conv2d(\n",
    "                num_channels,\n",
    "                base_model.features[0][0].out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                bias=False)\n",
    "        # Modify the classifier\n",
    "        in_features = base_model.classifier[2].in_features\n",
    "        base_model.classifier[2] = nn.Linear(in_features, 1)\n",
    "        model = base_model\n",
    "        # Use common transforms\n",
    "        train_transform = common_train_transform\n",
    "        val_test_transform = common_val_test_transform\n",
    "        input_size = (num_channels, 224, 224)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Architecture {architecture} not supported.\")\n",
    "    return model, train_transform, val_test_transform, input_size\n",
    "\n",
    "# ======================================\n",
    "# Part 3: Training, Evaluation, and Results Compilation\n",
    "# ======================================\n",
    "\n",
    "# ======================================\n",
    "# Training and Evaluation Functions\n",
    "# ======================================\n",
    "def train_and_evaluate(model, train_loader, val_loader,\n",
    "                       num_epochs=100, patience=15,\n",
    "                       checkpoint_path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates it on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        num_epochs (int): Maximum number of epochs.\n",
    "        patience (int): Early stopping patience.\n",
    "        checkpoint_path (str): Path to save the best model.\n",
    "\n",
    "    Returns:\n",
    "        train_losses (list): List of training losses per epoch.\n",
    "        val_losses (list): List of validation losses per epoch.\n",
    "        model (nn.Module): The best trained model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.3f}, \"\n",
    "              f\"Val Loss: {val_loss:.3f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "    return train_losses, val_losses, model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "\n",
    "    Returns:\n",
    "        mae (float): Mean Absolute Error.\n",
    "        rmse (float): Root Mean Squared Error.\n",
    "        r2 (float): R² Score.\n",
    "        predictions (np.array): Model predictions.\n",
    "        actuals (np.array): Actual labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).squeeze(1).cpu().numpy()\n",
    "            labels = labels.numpy()\n",
    "            predictions.extend(outputs)\n",
    "            actuals.extend(labels)\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    return mae, rmse, r2, predictions, actuals\n",
    "\n",
    "def get_model_complexity(model, input_size):\n",
    "    \"\"\"\n",
    "    Computes the total number of parameters and FLOPs of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_size (tuple): Size of the input tensor.\n",
    "\n",
    "    Returns:\n",
    "        total_params (int): Total number of parameters.\n",
    "        total_flops (int): Total number of floating-point operations.\n",
    "    \"\"\"\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, *input_size)\n",
    "    try:\n",
    "        flop_count = FlopCountAnalysis(model, dummy_input)\n",
    "        total_flops = flop_count.total()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating FLOPs: {e}\")\n",
    "        total_flops = None\n",
    "\n",
    "    # Calculate total parameters\n",
    "    total_params = parameter_count(model)['']\n",
    "    return total_params, total_flops\n",
    "\n",
    "# ======================================\n",
    "# Model Architectures to Train\n",
    "# ======================================\n",
    "architectures = [\n",
    "    # 'efficientnet_v2_s',\n",
    "    # 'efficientnet_v2_m',\n",
    "    # 'inception_v3',\n",
    "    'convnext_tiny',\n",
    "    'convnext_small'\n",
    "]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over architectures\n",
    "for architecture in architectures:\n",
    "    model_name = architecture.upper()\n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "\n",
    "    # Create the model and transforms\n",
    "    try:\n",
    "        model, train_transform, val_test_transform, input_size = create_model(\n",
    "            architecture, num_channels=3)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error creating model {model_name}: {ve}\")\n",
    "        continue\n",
    "\n",
    "    # Create Dataset Instances with appropriate transforms\n",
    "    train_dataset = TCIRLazyDataset(data_path, train_data_info, transform=train_transform)\n",
    "    val_dataset = TCIRLazyDataset(data_path, val_data_info, transform=val_test_transform)\n",
    "    test_dataset = TCIRLazyDataset(data_path, test_data_info, transform=val_test_transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True,\n",
    "        num_workers=3, pin_memory=True)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=32, shuffle=False,\n",
    "        num_workers=3, pin_memory=True)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=32, shuffle=False,\n",
    "        num_workers=3, pin_memory=True)\n",
    "\n",
    "    # Define checkpoint path\n",
    "    checkpoint_path = f\"best_model_{architecture}.pth\"\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, val_losses, best_model = train_and_evaluate(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=100, patience=15,\n",
    "        checkpoint_path=checkpoint_path)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating {model_name} on test set...\\n\")\n",
    "    mae, rmse, r2, predictions, actuals = evaluate_model(\n",
    "        best_model, test_loader)\n",
    "    print(f\"{model_name} - Test MAE: {mae:.3f}, \"\n",
    "          f\"RMSE: {rmse:.3f}, R2 Score: {r2:.3f}\")\n",
    "\n",
    "    # Compute FLOPs and Params\n",
    "    print(f\"\\nCalculating FLOPs and Parameters \"\n",
    "          f\"for {model_name}...\\n\")\n",
    "    total_params, total_flops = get_model_complexity(best_model, input_size)\n",
    "    if total_flops is not None:\n",
    "        flops_display = f\"{total_flops:,}\"\n",
    "    else:\n",
    "        flops_display = \"N/A\"\n",
    "    print(f\"{model_name} - Total Params: \"\n",
    "          f\"{total_params:,}, Total FLOPs: {flops_display}\")\n",
    "\n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame({\n",
    "        'Epoch': range(1, len(train_losses) + 1),\n",
    "        'Train Loss': train_losses,\n",
    "        'Val Loss': val_losses\n",
    "    })\n",
    "    history_csv_path = f\"training_history_{architecture}.csv\"\n",
    "    history_df.to_csv(history_csv_path, index=False)\n",
    "    print(f\"\\nTraining history saved to '{history_csv_path}'\")\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2 Score': r2,\n",
    "        'Total Params': total_params,\n",
    "        'Total FLOPs': total_flops\n",
    "    })\n",
    "\n",
    "# ======================================\n",
    "# Compile Results into a Table\n",
    "# ======================================\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal Results:\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df.to_csv('model_evaluation_results_experiment2_convnext.csv', index=False)\n",
    "print(\"\\nModel evaluation results saved to \"\n",
    "      \"'model_evaluation_results_experiment2_convnext.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5698083,
     "sourceId": 9390224,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37431.910543,
   "end_time": "2024-11-23T08:20:20.536350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-22T21:56:28.625807",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
